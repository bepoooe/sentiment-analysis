from flask import Flask, render_template, request, jsonify
import json
import os
from textblob import TextBlob
import pandas as pd
from datetime import datetime
import numpy as np
from collections import Counter
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
import joblib
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

app = Flask(__name__)

def get_available_data_files(folder_path='instagram_data'):
    """Get list of available data files with their paths."""
    base_path = os.path.dirname(os.path.abspath(__file__))
    folder_path = os.path.join(base_path, folder_path)
    available_files = []
    
    if os.path.exists(folder_path):
        for dir_name in os.listdir(folder_path):
            dir_path = os.path.join(folder_path, dir_name)
            if os.path.isdir(dir_path):
                # Extract account name and timestamp
                parts = dir_name.split('_')
                account = parts[0] if parts[0] != 'blu' else 'blu_es'
                timestamp = dir_name.split('__')[-1] if '__' in dir_name else '_'.join(parts[1:])
                json_file = f"{account}.json"
                json_path = os.path.join(dir_path, json_file)
                
                if os.path.exists(json_path):
                    display_name = f"{account} ({timestamp})"
                    available_files.append({
                        'path': json_path,
                        'display_name': display_name,
                        'account': account,
                        'timestamp': timestamp
                    })
    
    return sorted(available_files, key=lambda x: x['timestamp'], reverse=True)

def load_social_media_data(folder_path='instagram_data', platform='instagram', selected_file=None):
    """Enhanced data loading function that processes all JSON files in the folder structure."""
    base_path = os.path.dirname(os.path.abspath(__file__))
    folder_path = os.path.join(base_path, folder_path)
    print(f"Looking for data in: {folder_path}")
      all_data = []
    if not os.path.exists(folder_path):
        print(f"Error: Directory {folder_path} does not exist")
        return []
    
    try:
        if selected_file:
            # Load single selected file
            if os.path.exists(selected_file):
                print(f"Loading selected file: {selected_file}")
                with open(selected_file, 'r', encoding='utf-8') as f:
                    data = json.loads(f.read())
                    # Add platform and account info
                    dir_name = os.path.basename(os.path.dirname(selected_file))
                    account_name = dir_name.split('_')[0]
                    if account_name == 'blu':
                        account_name = 'blu_es'
                    
                    if isinstance(data, list):
                        for item in data:
                            item['platform'] = platform
                            item['account_name'] = account_name
                        all_data.extend(data)
                    else:
                        data['platform'] = platform
                        data['account_name'] = account_name
                        all_data.append(data)
            return all_data

        # If no file selected, load all files
        timestamped_dirs = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]
        print(f"Found directories: {timestamped_dirs}")
        
        for dir_name in timestamped_dirs:
            # Extract account name from directory name (e.g., 'blu_es' from 'blu_es__2025-05-01_15-40-46')
            account_name = dir_name.split('_')[0]
            if account_name == 'blu':  # Handle special case for blu_es
                account_name = 'blu_es'
            
            json_file = f"{account_name}.json"  # Construct JSON filename
            json_path = os.path.join(folder_path, dir_name, json_file)
            
            if os.path.exists(json_path):
                print(f"Processing file: {json_path}")
                try:
                    with open(json_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        data = json.loads(content)
                        
                        # Add platform and timestamp identifier
                        timestamp = dir_name.split('__')[-1] if '__' in dir_name else dir_name.split('_', 1)[1]
                        if isinstance(data, list):
                            for item in data:
                                item['platform'] = platform
                                item['account_name'] = account_name
                                if not item.get('timestamp'):
                                    # Convert folder timestamp format to ISO format
                                    ts = datetime.strptime(timestamp, '%Y-%m-%d_%H-%M-%S')
                                    item['timestamp'] = ts.strftime('%Y-%m-%dT%H:%M:%S.000Z')
                            all_data.extend(data)
                        else:
                            data['platform'] = platform
                            data['account_name'] = account_name
                            if not data.get('timestamp'):
                                ts = datetime.strptime(timestamp, '%Y-%m-%d_%H-%M-%S')
                                data['timestamp'] = ts.strftime('%Y-%m-%dT%H:%M:%S.000Z')
                            all_data.append(data)
                except Exception as e:
                    print(f"Error reading {json_path}: {str(e)}")
            else:
                print(f"File not found: {json_path}")
    except Exception as e:
        print(f"Error processing directory {folder_path}: {str(e)}")
    
    print(f"Loaded {len(all_data)} posts from {platform}")
    return all_data

def analyze_sentiment(text):
    """Calculate sentiment polarity of text."""
    if not text:
        return 0
    blob = TextBlob(str(text))
    return blob.sentiment.polarity

def analyze_engagement(post):
    """Calculate engagement score based on platform-specific metrics."""
    if post['platform'] == 'instagram':
        likes = post.get('likesCount', 0)
        comments = post.get('commentsCount', 0)
        if 'videoViewCount' in post:
            engagement_score = (likes + comments * 2 + post['videoViewCount']) / 100
        else:
            engagement_score = (likes + comments * 2) / 100
    else:  # YouTube or other platforms
        likes = post.get('likesCount', post.get('likeCount', 0))
        comments = post.get('commentsCount', post.get('commentCount', 0))
        engagement_score = (likes + comments * 2) / 100
    
    return engagement_score

def categorize_comment_type(text):
    """Categorize comments into different types."""
    if not text:
        return "no comment"
    
    text = text.lower()
    if any(emoji in text for emoji in ['‚ù§Ô∏è', 'üòç', 'üî•', 'üëè', 'üíô', 'üòä']):
        return "emoji positive"
    elif any(emoji in text for emoji in ['üò¢', 'üò≠', 'üíî', 'üòû', 'üò™']):
        return "emoji negative"
    elif len(text.split()) <= 3:
        return "short response"
    elif '?' in text:
        return "question"
    elif any(word in text for word in ['amazing', 'beautiful', 'awesome', 'great', 'love']):
        return "enthusiastic"
    elif any(word in text for word in ['fake', 'bad', 'terrible', 'hate', 'worst']):
        return "critical"
    else:
        return "general comment"

def process_instagram_data(data):
    """Process raw Instagram data into a structured DataFrame."""
    processed_data = []
    
    for post in data:
        # Process all comments for the post
        comments = post.get('comments', [])
        comment_sentiments = []
        comment_types = []
        
        # Get the first comment if available for high-level analysis
        first_comment = comments[0].get('text', '') if comments else ''
        first_comment_sentiment = analyze_sentiment(first_comment)
        
        # Process all comments for detailed analysis
        for comment in comments:
            comment_text = comment.get('text', '')
            if comment_text:
                sentiment = analyze_sentiment(comment_text)
                comment_type = categorize_comment_type(comment_text)
                comment_sentiments.append(sentiment)
                comment_types.append(comment_type)
        
        # Calculate comment sentiment stats
        avg_comment_sentiment = sum(comment_sentiments) / len(comment_sentiments) if comment_sentiments else 0
        sentiment_variance = np.var(comment_sentiments) if len(comment_sentiments) > 1 else 0
        
        # Count comment types
        type_counts = Counter(comment_types)
        total_comments = len(comment_types)
        
        # Parse timestamp
        try:
            timestamp = datetime.strptime(post.get('timestamp', post.get('createdAt', '')), '%Y-%m-%dT%H:%M:%S.000Z')
        except ValueError:
            timestamp = datetime.now()
        
        # Calculate engagement score
        engagement_score = analyze_engagement(post)
        
        processed_data.append({
            'post_id': post.get('id', ''),
            'timestamp': timestamp,
            'hour': timestamp.hour,
            'likes_count': post.get('likesCount', 0),
            'comments_count': post.get('commentsCount', 0),
            'caption': post.get('caption', ''),
            'caption_sentiment': analyze_sentiment(post.get('caption', '')),
            'first_comment': first_comment,
            'first_comment_sentiment': first_comment_sentiment,
            'avg_comment_sentiment': avg_comment_sentiment,
            'sentiment_variance': sentiment_variance,
            'emoji_positive_ratio': type_counts['emoji positive'] / total_comments if total_comments > 0 else 0,
            'emoji_negative_ratio': type_counts['emoji negative'] / total_comments if total_comments > 0 else 0,
            'enthusiastic_ratio': type_counts['enthusiastic'] / total_comments if total_comments > 0 else 0,
            'critical_ratio': type_counts['critical'] / total_comments if total_comments > 0 else 0,
            'question_ratio': type_counts['question'] / total_comments if total_comments > 0 else 0,
            'short_response_ratio': type_counts['short response'] / total_comments if total_comments > 0 else 0,
            'general_comment_ratio': type_counts['general comment'] / total_comments if total_comments > 0 else 0,
            'engagement_score': engagement_score,
            'type': post.get('type', 'Unknown'),
            'platform': post['platform']
        })
    
    return pd.DataFrame(processed_data)

def train_engagement_model(df):
    """Train a machine learning model to predict engagement."""
    # Prepare features for model training
    features = [
        'caption_sentiment', 'avg_comment_sentiment', 'sentiment_variance',
        'emoji_positive_ratio', 'emoji_negative_ratio', 'enthusiastic_ratio',
        'critical_ratio', 'question_ratio', 'short_response_ratio', 'general_comment_ratio'
    ]
    
    X = df[features]
    y = df['engagement_score']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # Train model
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    # Save model
    joblib.dump(model, 'engagement_model.joblib')
    
    # Calculate feature importance
    feature_importance = pd.DataFrame({
        'Feature': features,
        'Importance': model.feature_importances_
    }).sort_values('Importance', ascending=False)
    
    return model, feature_importance

def analyze_comment_patterns(comments):
    """Analyze patterns in comments."""
    patterns = Counter(comments)
    total = len(comments)
    insights = []
    
    for category, count in patterns.most_common():
        percentage = (count / total) * 100
        if percentage > 5:  # Only report significant patterns
            insights.append(f"- {category.title()}: {percentage:.1f}% of comments")
    
    return insights

def generate_engagement_story(df, account_name=None):
    """Generate insights about engagement timing."""
    stories = []
    
    if account_name:
        stories.append(f"\nüéØ Account Analysis: @{account_name}")
    
    # Analyze peak engagement times
    peak_hours = df.groupby('hour')['engagement_score'].mean().nlargest(3)
    
    stories.append("\n‚è∞ Peak Engagement Times:")
    for hour, score in peak_hours.items():
        time_str = f"{hour:02d}:00"
        stories.append(f"- {time_str} UTC with average engagement score of {score:.2f}")
    
    # Content type impact if available
    if 'type' in df.columns and df['type'].nunique() > 1:
        content_impact = df.groupby('type')['engagement_score'].agg(['mean', 'count'])
        stories.append("\nüìä Content Type Performance:")
        for content_type, stats in content_impact.iterrows():
            stories.append(f"- {content_type} posts ({int(stats['count'])} posts) average {stats['mean']:.2f} engagement points")
    
    # Sentiment correlation with engagement
    sentiment_corr = df['caption_sentiment'].corr(df['engagement_score'])
    comment_sent_corr = df['avg_comment_sentiment'].corr(df['engagement_score'])
    
    stories.append("\nüòä Sentiment Impact:")
    if abs(sentiment_corr) > 0.2:
        direction = "positive" if sentiment_corr > 0 else "negative"
        stories.append(f"- Posts with {direction} captions tend to get {abs(sentiment_corr):.2f}x more engagement")
    
    if abs(comment_sent_corr) > 0.2:
        direction = "positive" if comment_sent_corr > 0 else "negative"
        stories.append(f"- Posts that attract {direction} comments show {abs(comment_sent_corr):.2f}x more engagement")
    
    return "\n".join(stories)

def generate_natural_language_insights(df, account_name=None):
    """Generate comprehensive insights from the data."""
    insights = []
    
    # Overall sentiment
    avg_caption_sentiment = df['caption_sentiment'].mean()
    avg_comment_sentiment = df['avg_comment_sentiment'].mean()
    
    caption_sentiment_text = "positive" if avg_caption_sentiment > 0.1 else "negative" if avg_caption_sentiment < -0.1 else "neutral"
    comment_sentiment_text = "positive" if avg_comment_sentiment > 0.1 else "negative" if avg_comment_sentiment < -0.1 else "neutral"
    
    insights.append(f"üìù Content Analysis:")
    insights.append(f"- Your captions tend to be {caption_sentiment_text} (score: {avg_caption_sentiment:.2f})")
    insights.append(f"- Your audience responds with primarily {comment_sentiment_text} comments (score: {avg_comment_sentiment:.2f})")
    
    # Comment type analysis
    comment_type_columns = [
        'emoji_positive_ratio', 'emoji_negative_ratio', 'enthusiastic_ratio', 
        'critical_ratio', 'question_ratio', 'short_response_ratio', 'general_comment_ratio'
    ]
    comment_type_means = df[comment_type_columns].mean()
    top_type = comment_type_means.idxmax()
    top_ratio = comment_type_means.max() * 100
    
    type_name = top_type.replace('_ratio', '').replace('_', ' ')
    insights.append(f"\nüë• Audience Interaction:")
    insights.append(f"- Most common response type: {type_name} ({top_ratio:.1f}% of comments)")
    
    # Add engagement timing story
    insights.append(generate_engagement_story(df, account_name))
    
    # Add recommendations
    insights.append("\nüí° Recommendations:")
    
    # Time-based recommendation
    best_hour = df.groupby('hour')['engagement_score'].mean().idxmax()
    insights.append(f"- Try posting around {best_hour:02d}:00 UTC for maximum engagement")
    
    # Content sentiment recommendation
    if avg_caption_sentiment > 0 and df['caption_sentiment'].corr(df['engagement_score']) > 0.2:
        insights.append("- Keep your positive tone in captions, it resonates well with your audience")
    elif avg_caption_sentiment < 0 and df['caption_sentiment'].corr(df['engagement_score']) > 0.2:
        insights.append("- Your audience responds well to serious content - consider keeping this approach")
    else:
        insights.append("- Experiment with more emotive captions to see if engagement increases")
    
    return insights

def create_visualization_plots(df, account_name=None):
    """Create comprehensive visualization plots."""
    title_prefix = f"@{account_name} - " if account_name else ""
    
    # Create subplots
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Comment Sentiment Distribution', 
            'Comment Types', 
            'Engagement vs Sentiment', 
            'Engagement by Hour'
        )
    )
    
    # 1. Sentiment Distribution
    sentiment_hist = go.Histogram(
        x=df['avg_comment_sentiment'], 
        name='Comment Sentiment',
        nbinsx=20,
        marker_color='skyblue'
    )
    fig.add_trace(sentiment_hist, row=1, col=1)
    
    # 2. Comment Types Distribution
    type_columns = [
        'emoji_positive_ratio', 'emoji_negative_ratio', 
        'enthusiastic_ratio', 'critical_ratio', 'question_ratio'
    ]
    type_means = df[type_columns].mean() * 100  # Convert to percentages
    
    # Format labels for readability
    type_labels = [col.replace('_ratio', '').replace('_', ' ').title() for col in type_means.index]
    
    comment_types = go.Bar(
        x=type_labels, 
        y=type_means.values,
        name='Comment Types',
        marker_color='lightgreen'
    )
    fig.add_trace(comment_types, row=1, col=2)
    
    # 3. Engagement vs Sentiment Scatter
    engagement_scatter = go.Scatter(
        x=df['caption_sentiment'], 
        y=df['engagement_score'],
        mode='markers',
        name='Engagement vs Caption Sentiment',
        marker=dict(
            size=10,
            color=df['avg_comment_sentiment'],
            colorscale='RdBu',
            colorbar=dict(title='Comment Sentiment'),
            showscale=True
        )
    )
    fig.add_trace(engagement_scatter, row=2, col=1)
    
    # 4. Engagement by Hour
    hourly_engagement = df.groupby('hour')['engagement_score'].mean().reset_index()
    
    time_series = go.Bar(
        x=hourly_engagement['hour'], 
        y=hourly_engagement['engagement_score'],
        name='Engagement by Hour',
        marker_color='coral'
    )
    fig.add_trace(time_series, row=2, col=2)
    
    # Update layout
    fig.update_layout(
        height=800, 
        title_text=f"{title_prefix}Instagram Engagement Analysis",
        template="plotly_white"
    )
    
    # Update axes
    fig.update_xaxes(title_text="Sentiment Score", row=1, col=1)
    fig.update_yaxes(title_text="Number of Comments", row=1, col=1)
    
    fig.update_xaxes(title_text="Comment Type", row=1, col=2)
    fig.update_yaxes(title_text="Percentage", row=1, col=2)
    
    fig.update_xaxes(title_text="Caption Sentiment", row=2, col=1)
    fig.update_yaxes(title_text="Engagement Score", row=2, col=1)
    
    fig.update_xaxes(title_text="Hour of Day (UTC)", row=2, col=2)
    fig.update_yaxes(title_text="Average Engagement", row=2, col=2)
    
    return fig.to_html(full_html=False, include_plotlyjs=True)

@app.route('/')
def home():
    try:
        # Get available data files
        available_files = get_available_data_files()
        if not available_files:
            error_msg = "No Instagram data found. Please check if the instagram_data folder contains valid JSON files."
            print(error_msg)
            return render_template('index.html', error=error_msg, available_files=[])
        
        # Get selected file from query parameters or use the most recent one
        selected_file = request.args.get('file', None)
        if selected_file:
            selected_file_info = next((f for f in available_files if f['path'] == selected_file), None)
        else:
            selected_file_info = available_files[0]
            selected_file = selected_file_info['path']
        
        # Load and process Instagram data
        print(f"Starting data load process for {selected_file}...")
        raw_data = load_social_media_data(selected_file=selected_file)
        
        # Process the raw data
        df = process_instagram_data(raw_data)
        print(f"Successfully processed {len(df)} posts")
        
        # Get list of available accounts for filtering
        available_accounts = df['post_id'].str.split('_').str[0].unique() if 'post_id' in df.columns else []
          # Get account name from the selected file info
        account_name = selected_file_info['account'] if selected_file_info else None
        
        # Generate insights and visualizations
        print("Generating insights...")
        insights = generate_natural_language_insights(df, account_name)
        print(f"Generated insights")
        
        print("Creating visualizations...")
        plots_html = create_visualization_plots(df, account_name)
        print("Visualizations created")
          # Train the model if there's enough data
        if len(df) >= 10:  # Minimum threshold for training
            print("Training engagement model...")
            model, feature_importance = train_engagement_model(df)
            model_trained = True
            # Convert feature importance to list for template
            feature_imp_list = [
                {"name": row["Feature"].replace("_ratio", "").replace("_", " ").title(), 
                 "importance": f"{row['Importance']:.2f}"} 
                for _, row in feature_importance.iterrows()
            ]
        else:
            model_trained = False
            feature_imp_list = []

        return render_template('index.html', 
                            insights=insights,
                            plots_html=plots_html,
                            model_trained=model_trained,
                            feature_importance=feature_imp_list,
                            available_files=available_files,
                            selected_file=selected_file)
    except Exception as e:
        error_msg = f"An error occurred while processing the data: {str(e)}"
        print(error_msg)
        return render_template('index.html', error=error_msg)

@app.route('/train', methods=['POST'])
def train():
    try:
        raw_data = load_social_media_data()
        if len(raw_data) == 0:
            return jsonify({'error': 'No data found'})
        
        df = process_instagram_data(raw_data)
        model, feature_importance = train_engagement_model(df)
        
        # Convert feature importance to list for response
        feature_imp_list = [
            {"name": row["Feature"].replace("_ratio", "").replace("_", " ").title(), 
             "importance": float(row['Importance'])} 
            for _, row in feature_importance.iterrows()
        ]
        
        return jsonify({
            'message': 'Model trained successfully',
            'post_count': len(df),
            'feature_importance': feature_imp_list
        })
    except Exception as e:
        return jsonify({'error': str(e)})

@app.route('/analyze', methods=['POST'])
def analyze():
    text = request.form.get('text')
    if not text:
        return jsonify({'error': 'No text provided'})
    
    # Analyze the sentiment
    sentiment = analyze_sentiment(text)
    comment_type = categorize_comment_type(text)
    
    # Load the model
    try:
        model = joblib.load('engagement_model.joblib')
    except:
        return jsonify({
            'sentiment': sentiment,
            'sentiment_label': "positive" if sentiment > 0.1 else "negative" if sentiment < -0.1 else "neutral",
            'comment_type': comment_type,
            'message': 'Sentiment analysis completed, but engagement prediction model is not available yet'
        })
    
    # Prepare features for prediction
    features = pd.DataFrame({
        'caption_sentiment': [0],  # placeholder
        'avg_comment_sentiment': [sentiment],
        'sentiment_variance': [0],  # placeholder
        'emoji_positive_ratio': [1 if comment_type == 'emoji positive' else 0],
        'emoji_negative_ratio': [1 if comment_type == 'emoji negative' else 0],
        'enthusiastic_ratio': [1 if comment_type == 'enthusiastic' else 0],
        'critical_ratio': [1 if comment_type == 'critical' else 0],
        'question_ratio': [1 if comment_type == 'question' else 0],
        'short_response_ratio': [1 if comment_type == 'short response' else 0],
        'general_comment_ratio': [1 if comment_type == 'general comment' else 0]
    })
    
    # Predict engagement
    predicted_engagement = model.predict(features)[0]
    
    return jsonify({
        'sentiment': sentiment,
        'sentiment_label': "positive" if sentiment > 0.1 else "negative" if sentiment < -0.1 else "neutral",
        'comment_type': comment_type,
        'predicted_engagement': round(predicted_engagement, 2)
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)